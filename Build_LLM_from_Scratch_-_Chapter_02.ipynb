{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define options\n",
    "import logging\n",
    "from typing import List\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fp = \"data/the-verdict.txt\"\n",
    "do_download_data = False if os.path.exists(data_fp) else True\n",
    "\n",
    "l = logging.getLogger(__name__)\n",
    "l.setLevel(logging.INFO)\n",
    "l.info(f\"{do_download_data=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data(url:str=\"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\",\n",
    "                  file_path:str=\"the-verdict.txt\"):\n",
    "  l.info(f\"Downloading Data to {file_path=}\")\n",
    "  import urllib.request\n",
    "  urllib.request.urlretrieve(url, file_path)\n",
    "\n",
    "if do_download_data:\n",
    "  download_data(file_path=data_fp)\n",
    "\n",
    "# let's have a look at the data\n",
    "with open(data_fp, \"r+\") as f:\n",
    "  all_text = f.readlines()\n",
    "  l.info(all_text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing our text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tokenizer\n",
    "class SimpleTokenizerV1:\n",
    "  def __init__(self, vocab) -> None:\n",
    "    self.str_to_int = vocab\n",
    "    self.int_to_str = {i:s for s, i in vocab.items()}\n",
    "\n",
    "  def encode(self, text:str) -> List[int]:\n",
    "    preprocessed = re.split(r'([,.?!()\"\\']|--|\\s)', text)\n",
    "    preprocessed = [\n",
    "        item.strip() for item in preprocessed if item.strip()\n",
    "    ]\n",
    "    ids = [self.str_to_int[s] for s in preprocessed]\n",
    "    return ids\n",
    "\n",
    "  def decode(self, ids:List[int]) -> str:\n",
    "    text = \" \".join([self.int_to_str[id] for id in ids])\n",
    "    text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{',': 0, '--': 1, '.': 2, '?': 3, 'Hello': 4, 'Is': 5, 'a': 6, 'test': 7, 'this': 8, 'world': 9}\n"
     ]
    }
   ],
   "source": [
    "# test the tokenizer\n",
    "text = \"Hello, world. Is this-- a test?\"\n",
    "preprocessed = re.split(r'([,.:;?!\"()\\']|--|\\s)', text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "all_words = sorted(set(preprocessed))\n",
    "vocab = {t:idx for idx, t in enumerate(all_words)}\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello test. Is this a world?\n"
     ]
    }
   ],
   "source": [
    "test_text = \"Hello test. Is this a world?\"\n",
    "ids = tokenizer.encode(test_text)\n",
    "l.info(ids)\n",
    "print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now if we ask for an unknown word - we will get an error\n",
    "test_text = \"Hello Caroline. Is this a world?\"\n",
    "try:\n",
    "  ids = tokenizer.encode(test_text)\n",
    "except Exception as e:\n",
    "  l.info(f\"{e=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{',': 0, '--': 1, '.': 2, '?': 3, 'Hello': 4, 'Is': 5, 'a': 6, 'test': 7, 'this': 8, 'world': 9, '<|unk|>': 10, '<|endoftext|>': 11}\n"
     ]
    }
   ],
   "source": [
    "# we can remedy this by adding the unknown token token to the vocabulary. other\n",
    "# special tokens handle the concatenation of different corpora, and so on\n",
    "all_tokens = all_words\n",
    "all_tokens.extend([\"<|unk|>\", \"<|endoftext|>\"])\n",
    "vocab = {t:idx for idx, t in enumerate(all_tokens)}\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# equipped with this extended vocabulary, we can build an updated tokenizer that\n",
    "# accounts for the special tokens\n",
    "class SimpleTokenizerV2:\n",
    "  def __init__(self, vocab) -> None:\n",
    "    self.str_to_int = vocab\n",
    "    self.int_to_str = {i:s for s, i in vocab.items()}\n",
    "\n",
    "  def encode(self, text:str) -> List[int]:\n",
    "    preprocessed = re.split(r'([,.?!()\"\\']|--|\\s)', text)\n",
    "    preprocessed = [\n",
    "        item.strip() for item in preprocessed if item.strip()\n",
    "    ]\n",
    "    preprocessed = [item if item in self.str_to_int\n",
    "                    else \"<|unk|>\" for item in preprocessed]\n",
    "    ids = [self.str_to_int[s] for s in preprocessed]\n",
    "    return ids\n",
    "\n",
    "  def decode(self, ids:List[int]) -> str:\n",
    "    text = \" \".join([self.int_to_str[id] for id in ids])\n",
    "    text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now test that indeed we get the unknown token\n",
    "tokenizer =  SimpleTokenizerV2(vocab)\n",
    "test_text = \"Hello Caroline. Is this a world?\"\n",
    "try:\n",
    "  ids = tokenizer.encode(test_text)\n",
    "except Exception as e:\n",
    "  l.info(f\"{e=}\")\n",
    "l.info(ids)\n",
    "l.info(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.8.0-cp311-cp311-win_amd64.whl (884 kB)\n",
      "     ------------------------------------- 884.5/884.5 kB 18.6 MB/s eta 0:00:00\n",
      "Collecting regex>=2022.1.18\n",
      "  Downloading regex-2024.11.6-cp311-cp311-win_amd64.whl (274 kB)\n",
      "     ---------------------------------------- 274.1/274.1 kB ? eta 0:00:00\n",
      "Collecting requests>=2.26.0\n",
      "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "     ---------------------------------------- 64.9/64.9 kB 3.6 MB/s eta 0:00:00\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.4.0-cp311-cp311-win_amd64.whl (101 kB)\n",
      "     -------------------------------------- 101.8/101.8 kB 5.7 MB/s eta 0:00:00\n",
      "Collecting idna<4,>=2.5\n",
      "  Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "     ---------------------------------------- 70.4/70.4 kB ? eta 0:00:00\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Downloading urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "     -------------------------------------- 126.3/126.3 kB 7.3 MB/s eta 0:00:00\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
      "     ------------------------------------- 167.3/167.3 kB 10.5 MB/s eta 0:00:00\n",
      "Installing collected packages: urllib3, regex, idna, charset-normalizer, certifi, requests, tiktoken\n",
      "Successfully installed certifi-2024.8.30 charset-normalizer-3.4.0 idna-3.10 regex-2024.11.6 requests-2.32.3 tiktoken-0.8.0 urllib3-2.2.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# !pip install tiktoken\n",
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see how this is different\n",
    "tokens = tokenizer.encode(test_text)\n",
    "l.info(f\"{tokens=}\")\n",
    "l.info(tokenizer.decode(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n"
     ]
    }
   ],
   "source": [
    "# remember we have the data in all_text\n",
    "the_text = \"\".join(all_text)\n",
    "encoded_text = tokenizer.encode(the_text)\n",
    "print(len(encoded_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to build our model which does next token prediction, we need to sample\n",
    "# up until the current token and then the next token as target\n",
    "# for demonstration purposes, we do this for context_size window\n",
    "context_size = 4\n",
    "for i in range(1, context_size + 1):\n",
    "  x = encoded_text[:i]\n",
    "  xhat = tokenizer.decode(x)\n",
    "  y = encoded_text[i]\n",
    "  yhat = tokenizer.decode([y])\n",
    "  l.info(f\"{xhat=} --> {yhat=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.5.1-cp311-cp311-win_amd64.whl (203.1 MB)\n",
      "     -------------------------------------- 203.1/203.1 MB 3.1 MB/s eta 0:00:00\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\chris\\development\\build_an_llm_from_scratch\\.venv\\lib\\site-packages (from torch) (4.12.2)\n",
      "Collecting networkx\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "     ---------------------------------------- 1.7/1.7 MB 12.2 MB/s eta 0:00:00\n",
      "Collecting jinja2\n",
      "  Using cached jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "Collecting fsspec\n",
      "  Downloading fsspec-2024.10.0-py3-none-any.whl (179 kB)\n",
      "     ------------------------------------- 179.6/179.6 kB 11.3 MB/s eta 0:00:00\n",
      "Collecting sympy==1.13.1\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "     ---------------------------------------- 6.2/6.2 MB 18.0 MB/s eta 0:00:00\n",
      "Collecting mpmath<1.4,>=1.1.0\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Downloading MarkupSafe-3.0.2-cp311-cp311-win_amd64.whl (15 kB)\n",
      "Installing collected packages: mpmath, sympy, networkx, MarkupSafe, fsspec, filelock, jinja2, torch\n",
      "Successfully installed MarkupSafe-3.0.2 filelock-3.16.1 fsspec-2024.10.0 jinja2-3.1.4 mpmath-1.3.0 networkx-3.4.2 sympy-1.13.1 torch-2.5.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chris\\Development\\Build_an_LLM_from_Scratch\\.venv\\Lib\\site-packages\\torch\\_subclasses\\functional_tensor.py:295: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "# now implement a torch data loader\n",
    "!pip install torch\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDatasetV1(Dataset):\n",
    "  def __init__(self, txt, tokenizer, max_length, stride) -> None:\n",
    "    self.input_ids = []\n",
    "    self.target_ids = []\n",
    "    token_ids = tokenizer.encode(txt)\n",
    "\n",
    "    for i in range(0, len(token_ids) - max_length, stride):\n",
    "      input_chunk = token_ids[i:i + max_length]\n",
    "      target_chunk = token_ids[i + 1:i + max_length + 1]\n",
    "      self.input_ids.append(torch.tensor(input_chunk))\n",
    "      self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.input_ids)\n",
    "  \n",
    "  def __getitem__(self, idx):\n",
    "    return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now create a data loader for this data set\n",
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, stride=128, \n",
    "                         shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "  tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "  dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "  dataloader = DataLoader(dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=shuffle,\n",
    "                          drop_last=drop_last,\n",
    "                          num_workers = num_workers\n",
    "  )\n",
    "  \n",
    "  return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so let's load\n",
    "dl = create_dataloader_v1(the_text, batch_size=1, max_length=4, stride=1, shuffle=False)\n",
    "data_iter = iter(dl)\n",
    "first_batch = next(data_iter)\n",
    "l.info(first_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embed the tokenized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens are embedded using a token embedding.\n",
    "vocab_size = 50257 # from BPE\n",
    "output_dim = 256   # embedding size\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "\n",
    "# to preserve the position a positional embedding - relative or absolute\n",
    "# is used. here we use an absolute position encoding scheme\n",
    "max_length = 4\n",
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "# we fill the positional embeddings - with an embedding of 0,1,..,context_length\n",
    "# to designate the position of each token\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
    "l.info(pos_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to test the embedding get some token from the data loader\n",
    "x, y = next(data_iter)\n",
    "l.info(f\"{x=} => {y=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and embedd\n",
    "token_embed = token_embedding_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# position embedding is simply 0, 1, 2, ... n - only the token embedding changes\n",
    "x_embed = token_embed + pos_embeddings\n",
    "l.info(x_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
